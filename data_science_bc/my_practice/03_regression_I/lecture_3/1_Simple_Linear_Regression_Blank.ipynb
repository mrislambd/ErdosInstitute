{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52da8da7",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "We start our section on regression with the simplest model, simple linear regression.\n",
    "\n",
    "## What we will accomplish in this notebook\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the simple linear regression model,\n",
    "- Discuss and visualize its assumptions,\n",
    "- Demonstrate how to fit the model theoretically and practically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b710adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13f7a5",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "In simple linear regression (SLR) we have a variable we would like to predict, $y$, and a single feature $x$. The form of $f$ in the supervised learning framework we have discussed is as follows:\n",
    "\n",
    "$$\n",
    "y = f(x) + \\epsilon = \\beta_0 + \\beta_1 x + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\beta_0, \\beta_1 \\in \\mathbb{R}$ are constants we must estimate and we assume that $\\epsilon \\sim N(0,\\sigma^2)$ is a normally distributed error term independent of $x$.\n",
    "\n",
    "### Visualizing the model\n",
    "\n",
    "Let's think about what this model is saying about the outcome variable, $y$. For help we will look at the picture drawn below.\n",
    "\n",
    "<img src=\"lecture_3_assets/slr_curves.png\" width=\"60%\"></img>\n",
    "\n",
    "Above we see both the systematic part and the random error. For a given value of $x$ you can find the theoretically possible values for $y$ by going to the line $\\beta_0 + \\beta_1 x$ and randomly drawing an error term from the normal distribution centered on the line. We can also see one of our key assumptions at play: no matter what the value of $x$, our errors are drawn from the same exact bell curve.\n",
    "\n",
    "You can look at a 3D version of the same diagram [here](https://www.desmos.com/3d/09db6f9c8d).\n",
    "\n",
    "If our assumptions hold, we can derive some nice features about estimates and predictions made in the course of fitting this model that we may touch on in our problem session and/or the homework.\n",
    "\n",
    "### Fitting the model\n",
    "\n",
    "Given $n$ observations of pairs $(x_i,y_i)$, $i = 1,\\dots,n$ how do we fit this model, what do we need to estimate? Remember that our goal is to find an estimate of $f$ called $\\hat{f}$. For SLR this means that we need to estimate $\\beta_0$ and $\\beta_1$, i.e. we need to find $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$.\n",
    "\n",
    "#### Minimizing mean square error (MSE)\n",
    "\n",
    "We find a $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ by minimizing a <i>loss function</i>, namely the mean square error (MSE), which is given by:\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(\\beta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - f_\\beta(x_i))^2.\n",
    "$$\n",
    "\n",
    "For the particular case of SLR this is:\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(\\beta_0,\\beta_1) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2.\n",
    "$$\n",
    "\n",
    "The MSE represents the average square error of the estimate from the actual value, for a measurement of the average error that is on the same scale as $y$ you can take the square root of the MSE known as the Root MSE or RMSE.\n",
    "\n",
    "You can look at a 2D visualization of the MSE at [this link](https://www.desmos.com/calculator/ewqexkfjm1) and a 3D visualization of the MSE at [this link](https://www.desmos.com/3d/72e4cb5e40).\n",
    "\n",
    "You can do some mathematics to find the values $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ that minimize the MSE.  This was covered in math hour.  The 5 second summary is that you can either:\n",
    "\n",
    "* Use calculus:  take the gradient of the MSE with respect to the parameters and set it equal to zero.\n",
    "* Use linear algebra:  use dot products to project $\\vec{y}$ onto the subspace spanned by $\\vec{1}$ and $\\vec{x}$\n",
    "\n",
    "Either way you do it we find that\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_0} = \\overline{y} - \\hat{\\beta_1} \\overline{x}, \\text{ and}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n \\left( x_i - \\overline{x}\\right)\\left( y_i - \\overline{y} \\right)}{\\sum_{i=1}^n \\left(x_i - \\overline{x} \\right)^2} = \\frac{\\text{cov}(x,y)}{\\text{var}(x)},\n",
    "$$\n",
    "\n",
    "where $\\overline{x}$ and $\\overline{y}$ are the means of $x$ and $y$ respectively, $\\text{cov}$ denotes the sample covariance and $\\text{var}$ denotes the sample variance.\n",
    "\n",
    "<i>Note:</i> MSE is used as the default loss function for simple linear regression for a number of reasons stemming from its roots as a statistical regression technique. Importantly, MSE is differentiable with respect to $\\beta_i$ and is a convex function. As seen in math hour we also minimize the MSE when performing a maximum likelihood estimate of the parameters.  However, MSE is not the only loss function people consider in this type of model. Check out the corresponding `Optional Extra Practice` notebook to learn about mean absolute error (MAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898b2e3",
   "metadata": {},
   "source": [
    "## Implementing SLR in `sklearn`\n",
    "\n",
    "While we can code up coefficient estimates for SLR using the formulae we just derived, we can also use `sklearn`'s `LinearRegression` model object.\n",
    "\n",
    "Here is the documentation for `LinearRegression`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</a>. We will show how to fit the model with some randomly generated data, but in the next notebook we will work with some real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ffc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making some data\n",
    "np.random.seed(321)\n",
    "X = np.random.random(100)\n",
    "y = 2*X + 1 + .5*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a71d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.scatter(X, y)\n",
    "\n",
    "plt.xlabel(\"$X$\", fontsize=12)\n",
    "plt.ylabel(\"$y$\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12524231",
   "metadata": {},
   "source": [
    "`sklearn` is <b>the</b> python machine learning model package. We will use it frequently throughout these notebooks. `sklearn` models follow a similar pattern we will now demonstrate.\n",
    "\n",
    "##### Import the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bba41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we import the model class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d84c7",
   "metadata": {},
   "source": [
    "##### Make a model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3693681",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we make an instance of the model\n",
    "## To do this just call the name of the model class, LinearRegression()\n",
    "## Sometimes there are optional arguments, here we note that copy_X  and fit_intercept default to True\n",
    "## this ensures that the X array is hard copied prior to fitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfad672",
   "metadata": {},
   "source": [
    "##### `fit`ting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5486b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we fit the model\n",
    "## this is typically model.fit(X, y)\n",
    "## NOTE! X has to be a 2D array, think matrix or column vector\n",
    "## Thus we must use .reshape(-1,1), see the Python Prep numpy notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905dfa85",
   "metadata": {},
   "source": [
    "##### Making `predict`ions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model.predict will tell us what the model says\n",
    "## for an array of input values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d763262",
   "metadata": {},
   "source": [
    "Those are the basic steps for most every `sklearn` model we will work with. However, models typically have features and methods that are unique to them. We will review a few of those for `LinearRegression` below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34249621",
   "metadata": {},
   "source": [
    "##### Simple linear regression content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can look at beta_0 with .intercept_\n",
    "slr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can look at beta_1 with .coef_\n",
    "slr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebcbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the model with our sample\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.scatter(X, \n",
    "            y, \n",
    "            alpha=.7,\n",
    "            label=\"Sample\")\n",
    "\n",
    "plt.plot(np.linspace(0, 1, 100),\n",
    "         slr.predict(np.linspace(0, 1, 100).reshape(-1,1)),\n",
    "         'k',\n",
    "         label='Model $\\hat{f}$')\n",
    "\n",
    "plt.plot(np.linspace(0,1,100),\n",
    "         2*np.linspace(0,1,100) + 1,\n",
    "         'r--',\n",
    "         label = \"$f$\")\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel(\"$X$\", fontsize=12)\n",
    "plt.ylabel(\"$y$\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b25df0",
   "metadata": {},
   "source": [
    "Now you know the basics about simple linear regression and `LinearRegression` in `sklearn`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb119dc",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
