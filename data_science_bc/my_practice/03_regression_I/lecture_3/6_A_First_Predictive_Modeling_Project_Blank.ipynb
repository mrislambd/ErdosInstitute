{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93054cbb",
   "metadata": {},
   "source": [
    "# A First Predictive Modeling Project\n",
    "\n",
    "In this notebook we will use the simple linear regression model to demonstrate the process of predictive model selection in a vaguely realistic scenario. \n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In particular we will:\n",
    "- Review some of the common steps in a predictive modeling project,\n",
    "- Work with a baseball data set,\n",
    "- Introduce the concept of a baseline model and\n",
    "- Practice implementing cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95baec9",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdda2f1",
   "metadata": {},
   "source": [
    "Common steps in a predictive modeling project include:\n",
    "\n",
    "1. Data Gathering, defining stakeholders and KPIs\n",
    "2. Data Cleaning and Preprocessing\n",
    "3. Exploratory Data Analysis\n",
    "4. Compare different models on the training set\n",
    "5. Final sanity check on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353dc9f6",
   "metadata": {},
   "source": [
    "## Example: A baseball regression problem\n",
    "\n",
    "#### Step 1. Data Gathering, defining stakeholders and KPIs\n",
    "\n",
    "Let's imagine we work for a major league baseball team in the off-season. During the off-season teams are looking to see what players they can bring in to improve upon their number of wins in the coming season.\n",
    "\n",
    "A reasonable question is whether stocking up on good defensive players (limiting the number of runs your team allows) or stocking up on good offensive players (increasing the number of runs your teams scores) is better at predicting the number of wins you will have in a given season. We could use existing projections for each player to estimate our number of runs scores and runs allowed after acquiring said player to predict our expected wins.\n",
    "\n",
    "In this problem we will do just that. First let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd28035",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Note this works on Mac and Linux,\n",
    "## you may need to change the slash directions if\n",
    "## you are running a Windows machine\n",
    "baseball = pd.read_csv(\"../../data/baseball.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a9097",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "baseball.sample(5, random_state=234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fcced1",
   "metadata": {},
   "source": [
    "This data set contains the wins (`W`), losses (`L`), runs scored (`R`) and runs allowed (`RA`) for each team (`teamID`) over the 2001-2018 seasons (`yearID`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d38b9d9",
   "metadata": {},
   "source": [
    "#### Step 2:  Data Cleaning and Preprocessing\n",
    "\n",
    "Check to make sure we don't have any missing values, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f66f4",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "baseball.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9ed98",
   "metadata": {},
   "source": [
    "This dataset is extremely clean!  Almost all real data science projects do not start with clean data.  Expect data cleaning and preprocessing to take up a significant portion of your time on a real project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c1682",
   "metadata": {},
   "source": [
    "Let's recall what we learned in `Data Splits for Predictive Modeling`, and make a train test split with $80\\%$ of the data in the training set and $20\\%$ of the data in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88af3b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237aa62",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## make the train test split here\n",
    "## Note a slight difference, we have to use .copy()\n",
    "## for pandas dataframes\n",
    "bb_train, bb_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38b19e",
   "metadata": {},
   "source": [
    "#### Step 3. Exploratory data analysis\n",
    "\n",
    "Prior to picking potential models we will do a quick data exploration by plotting `W` against `R` and `RA` respectively. Note that we chose these plots because these are the only variables that a team may be able to change by signing players to contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fad84c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 2, figsize=(12,5), sharey=True)\n",
    "\n",
    "ax[0].scatter(bb_train.R, \n",
    "              bb_train.W)\n",
    "ax[0].set_ylabel(\"Wins\", fontsize=12)\n",
    "ax[0].set_xlabel(\"Runs Scored\", fontsize=12)\n",
    "\n",
    "\n",
    "ax[1].scatter(bb_train.RA,\n",
    "              bb_train.W)\n",
    "ax[1].set_xlabel(\"Runs Allowed\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c882f",
   "metadata": {},
   "source": [
    "Both of these look to be linear relationships, which is an important check for our choice in simple linear regression. If it did not look as though there was a linear relationship, we would not use simple linear regression. \n",
    "\n",
    "In real modeling projects there is more data exploration, but for this notebook we will continue on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c610be",
   "metadata": {},
   "source": [
    "#### Step 4. Comparing different models on the training set\n",
    "\n",
    "We will try the following models\n",
    "\n",
    "$$\n",
    "\\text{Model 1: } \\ \\ \\texttt{W} = \\beta_0 + \\beta_1 \\texttt{R} + \\epsilon,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Model 2: } \\ \\ \\texttt{W} = \\beta_0 + \\beta_1 \\texttt{RA} + \\epsilon.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Model 3: } \\ \\ \\texttt{W} = \\beta_0 + \\beta_1 \\texttt{R} + \\beta_2 \\texttt{RA} + \\epsilon.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Model 4: kNN using R and RA as features with k=10} \n",
    "$$\n",
    "\n",
    "Note 1:  We would usually perform some form of hyperparameter tuning to decide on the value of $k$ in kNN but we are simplifying things for illustrative purposes.\n",
    "\n",
    "Note 2: kNN might be good at prediction, but a downside is that we will loose interpret-ability and the ability to extrapolate to more unusual input values.\n",
    "\n",
    "##### Identifying a baseline model\n",
    "\n",
    "In predictive modeling problems it is also smart to have a <i>baseline model</i>. A baseline model is a simple model that exists for comparison purposes. These are important because they allow us to put our model results into context. For example, we may end up with an MSE of $100$. Is that good? Is that bad? In the abstract it is impossible to tell. It is only when we have a reasonable baseline model that we are able to put into context how good our performance is. Again for example, if our baseline model's MSE was $1000$ our model with MSE$=100$ has done quite well, but if our baseline model's MSE was $10$ our model has underperformed the baseline.\n",
    "\n",
    "Moreover, merely outperforming the baseline is not always an indication of the best model. We also need to consider things like training time, model complexity and interpretability. While those will not be considerations in this notebook, they will become more important as we progress through these notebooks and in your own data science projects.\n",
    "\n",
    "For this problem a standard first baseline is to predict the average value of the output, $\\overline{\\texttt{W}}$, for any value of the input. This brings us to three models:\n",
    "\n",
    "$$\n",
    "\\text{Model 0: } \\ \\ \\texttt{W} = E\\left(\\texttt{W}\\right) + \\epsilon.\n",
    "$$\n",
    "\n",
    "Now that we have our models, let's implement cross-validation. The code below will have missing pieces, I encourage you to try and fill them out on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93997193",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6138f2",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Make a KFold object with k=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2f8eb",
   "metadata": {},
   "source": [
    "We will look for the model with the lowest mean square error. We could calculate this by hand, but we will use `sklearn`'s `mean_squared_error` instead, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0bb632",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## importing mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a033c12",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## import LinearRegression and KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08739c7",
   "metadata": {},
   "source": [
    "Now we are ready to perform cross-valdiation. Again there will be some empty spots that you can fill in on your own if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28460545",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## make an array of zeros that will hold our mses\n",
    "mses = np.zeros((5, 5))\n",
    "\n",
    "## fill in what is missing in the for loop declaration\n",
    "## Note that using \"enumerate\" is slightly more \"pythonic\" than incrementing i manually.\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(bb_train)):\n",
    "    ## now we get the training splits and the holdout split\n",
    "    ### Training\n",
    "    bb_tt = \n",
    "    \n",
    "    ### Holdout set\n",
    "    bb_ho = \n",
    "    \n",
    "    \n",
    "    ## take the mean W from the training set\n",
    "    ## we need predictions for the entire holdout set\n",
    "    pred0 = \n",
    "    \n",
    "\n",
    "    model1 = \n",
    "    model2 =\n",
    "    model3 = \n",
    "    model4 = \n",
    "\n",
    "    ## fit models on the training data, bb_tt\n",
    "    ## don't forget you may need to reshape the data for simple linear regressions\n",
    "\n",
    "    # No need to reshape inputs for model3 or model4\n",
    "\n",
    "    \n",
    "    ## get the prediction on holdout set\n",
    "    pred1 = \n",
    "    pred2 = \n",
    "    pred3 = \n",
    "    pred4 = \n",
    "    \n",
    "    ### Recording the MSES ###\n",
    "    ## mean_squared_error takes in the true values, then the predicted values\n",
    "    mses[0,i] = mean_squared_error(bb_ho.W.values, pred0)\n",
    "    mses[1,i] = mean_squared_error(bb_ho.W.values, pred1)\n",
    "    mses[2,i] = mean_squared_error(bb_ho.W.values, pred2)\n",
    "    mses[3,i] = mean_squared_error(bb_ho.W.values, pred3)\n",
    "    mses[4,i] = mean_squared_error(bb_ho.W.values, pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad359908",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## This figure will compare the performance\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.scatter(np.zeros(5), \n",
    "            mses[0,:], \n",
    "            s=60, \n",
    "            c='white',\n",
    "            edgecolor='black',\n",
    "            label=\"Single Split\")\n",
    "plt.scatter(np.ones(5), \n",
    "            mses[1,:], \n",
    "            s=60, \n",
    "            c='white',\n",
    "            edgecolor='black')\n",
    "plt.scatter(2*np.ones(5), \n",
    "            mses[2,:], \n",
    "            s=60, \n",
    "            c='white',\n",
    "            edgecolor='black')\n",
    "\n",
    "plt.scatter(3*np.ones(5), \n",
    "            mses[3,:], \n",
    "            s=60, \n",
    "            c='white',\n",
    "            edgecolor='black')\n",
    "\n",
    "plt.scatter(4*np.ones(5), \n",
    "            mses[3,:], \n",
    "            s=60, \n",
    "            c='white',\n",
    "            edgecolor='black')\n",
    "\n",
    "plt.scatter([0,1,2,3,4], \n",
    "            np.mean(mses, axis=1), \n",
    "            s=60, \n",
    "            c='r',\n",
    "            marker='X',\n",
    "            label=\"Mean\")\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.xticks([0,1,2,3, 4],[\"Baseline\", \"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"], fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "plt.ylabel(\"MSE\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16902f1",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "np.mean(mses, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3d782",
   "metadata": {},
   "source": [
    "From this it appears that Model 3 is our best choice as it has the lowest average cross-validation MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3759a8",
   "metadata": {},
   "source": [
    "#### Step 5. Final sanity check on test set\n",
    "\n",
    "Remember that you **only test your final model on the test set**.  Don't try your other models on the test set.\n",
    "\n",
    "Once you are done tinkering and have settled on a final model (or a final set of a few models) you can perform your final check with the test set. The purposes of this check are twofold:\n",
    "1. We will be able to see if we had any coding errors in our work up to this point, performance that greatly departs from what we would expect may indicate a coding error in our earlier work and\n",
    "2. It allows us to assess overfitting (more on this soon). When our test performance is significantly worse than our training performance it suggests that our model has overfit on the training data and will not generalize well.\n",
    "\n",
    "If you notice weird behavior on the test set it may be worth reviewing your previous work and checking out different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48030df1",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Make a final model object\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f7c3d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## fit that on the entire training set\n",
    "model.fit(bb_train[['R','RA']], bb_train.W.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785c463",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## print the training set performance\n",
    "print(\"Training set RMSE:\", \n",
    "      np.round(np.sqrt(mean_squared_error(bb_train.W.values, model.predict(bb_train[['R','RA']]))),2))\n",
    "\n",
    "\n",
    "## print the test set performance\n",
    "print(\"Test set RMSE:\", \n",
    "      np.round(np.sqrt(mean_squared_error(bb_test.W.values, model.predict(bb_test[['R','RA']]))),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf98ff3",
   "metadata": {},
   "source": [
    "These are comparable performances, so I think we are in the clear for over-fitting and coding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd20cb",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Interpretation of the model\n",
    "print(f\"Each additional run scored increases our predicted number of wins by {model.coef_[0]}\")\n",
    "print(f\"Each additional run allowed decreases our predicted number of wins by {-model.coef_[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86714c2f",
   "metadata": {},
   "source": [
    "These are really close so we cannot clearly say whether we should focus more on offense or defense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91144169",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
