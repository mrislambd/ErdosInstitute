{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d049c4b5",
   "metadata": {},
   "source": [
    "# Data Splits\n",
    "\n",
    "We want our models to generalize well to unseen data. To that end we\n",
    "\n",
    "* Split our data into **training** and **testing** sets.  \n",
    "    * We never do anything with the testing set until the **very end of our work** as a final sanity check.\n",
    "* During model selection we further split our training set using either\n",
    "    * A single **validation** set or\n",
    "    * A k-fold **cross-validation** split\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss the rationale for splitting our data set\n",
    "- Introduce train test splits, validation sets, and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062831f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will now start importing a common set\n",
    "## of items at the onset of most notebooks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a0e44",
   "metadata": {},
   "source": [
    "## Data splits guard against over-fitting\n",
    "\n",
    "Over-fitting is when a model fits too closely to the data it was trained on and does not generalize to new data as well as it otherwise could.\n",
    "\n",
    "We will give a more formal presentation in lecture 5 (the \"Bias/Variance Tradeoff\" notebook) but we need at least an informal understanding immediately.\n",
    "\n",
    "<img src=\"lecture_3_assets/overfit.png\"></img>\n",
    "\n",
    "The 2nd model is over-fitting:  we can see that it would not generalize well to new data which follows the same distribution as our training data.\n",
    "\n",
    "It was easy to see that we are over-fitting here because the relationship is relatively simple and the data is low dimensional enough that we can visualize it.  When we are dealing with real data we might have hundreds of features, and simple visual checks would not be sufficient.\n",
    "\n",
    "One of the best ways to guard against over-fitting is to use a **data split**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032dcf34",
   "metadata": {},
   "source": [
    "## Train test splits\n",
    "\n",
    "The first split we will touch on is the first split you would do in a new data science project: the **train test split**.\n",
    "\n",
    "The purpose of the train test split is to create two data sets:\n",
    "1. <b>The training set</b> - This subset is used to fit models and compare model candidates. This data set is usually split further.\n",
    "2. <b>The testing set</b> - This subset is used as a final check on your selected model prior to putting your model into its desired final state.\n",
    "\n",
    "The training set usually contains the majority of the original data. Common train test split percentage divisions are $80\\% - 20\\%$ or $75\\% - 25\\%$, but it may sometimes be appropriate to use different split sizes. Train test splits are done randomly, with the form of randomness dependent upon your project.\n",
    "\n",
    "Here is an illustration of a train test split:\n",
    "\n",
    "<img src=\"lecture_3_assets/train_test.png\" width=\"40%\"></img>\n",
    "\n",
    "<b>IMPORTANT:  The test set is not directly used to compare models</b>\n",
    "\n",
    "Model comparison is typically done using further splits of the training set. \n",
    "\n",
    "It is embarrassing and costly to ship a product which doesn't perform well on novel data.  The test set serves as a **final sanity check** on your work before sending it out into the world.\n",
    "\n",
    "### Performing train test splits in `sklearn`\n",
    "\n",
    "The `sklearn` package has a useful `train_test_split` function that will perform the train test split. Here is a link to the documentation:\n",
    "\n",
    " <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we will make a data set\n",
    "X = np.random.random((1000,10))\n",
    "y = np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d47a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we make the split\n",
    "## train_test_split returns 4 outputs: X_train, X_test, y_train and y_test\n",
    "##\n",
    "## First you input the X and y for your data\n",
    "##\n",
    "## then set the shuffle argument to True, this randomly shuffles the\n",
    "## data before it is split\n",
    "##\n",
    "## The random_state ensures that the random split is the same each time\n",
    "## someone runs the code chunk, it can be any strictly positive integer\n",
    "##\n",
    "## You can specify the size of the test set with test_size,\n",
    "## here I want 20% of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the data lengths to see that they match\n",
    "## what we'd expect\n",
    "print(\"The shape of X_train is\",X_train.shape)\n",
    "print(\"The shape of X_test is\",X_test.shape)\n",
    "print(\"The length of y_train is\",len(y_train))\n",
    "print(\"The length of y_test is\",len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a47dfeb",
   "metadata": {},
   "source": [
    "## Two split types for model comparison and selection\n",
    "\n",
    "We will now cover two data splits you can make from the training set for model comparison purposes. Which you choose depends upon the project you are working on, but we will give some reasons to choose one over the other below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56435013",
   "metadata": {},
   "source": [
    "### Validation sets\n",
    "\n",
    "A <i>validation set</i> is a subset of the training data (the result of the train test split defined above) used solely for the purpose of comparing candidate models. This split is typically also performed randomly. Further, the validation set should be a small subset, common sizes range from $10\\%-25\\%$ of the training set depending on the training set size. An illustration of this concept is given below:\n",
    "\n",
    "<img src=\"lecture_3_assets/validation_set.png\" width=\"45%\"></img>\n",
    "\n",
    "The best model in this setting would be the one that has the best performance metric on the validation set.\n",
    "\n",
    "#### In practice\n",
    "\n",
    "In practice we can once again use `sklearn`'s `train_test_split` function to make the validation split. Note that it is good practice to not overwrite the original `X_train` or `y_train` sets when making the validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b869318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we make a validation set with 15% of the \n",
    "## training data in the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419372cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(\"15% of\",800,\"is\",.15*800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b78400",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train_train\", X_train_train.shape)\n",
    "print(\"Shape of X_val\", X_val.shape)\n",
    "print(\"Length of y_train_train\", len(y_train_train))\n",
    "print(\"Length of y_val\", len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346d703",
   "metadata": {},
   "source": [
    "### $k$-Fold cross-validation\n",
    "\n",
    "The validation set approach only gives us one check on how well our model generalizes.  We might get unusually lucky or unlucky with this one check.  $k$-fold cross validation gives us $k$ opportunities to see how well our model will generalize instead of just one.\n",
    "\n",
    "<img src=\"lecture_3_assets/cv1.png\" width=\"60%\"></img>\n",
    "\n",
    "Common values for $k$ are between $5$ and $10$.  \"Leave out one\" cross validation is another strategy which is equivalent to taking $k = n-1$ where $n$ is the number of samples in your training data.\n",
    "\n",
    "You can implement cross-validation using `sklearn`'s `KFold` object. Documentation for this method can be found here \n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe87b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a KFold object\n",
    "## n_splits controls the value of k\n",
    "## shuffle=True, randomly shuffles the data prior to splitting\n",
    "## random_state is the same as for train_test_split\n",
    "kfold = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrate.split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cbe56",
   "metadata": {},
   "source": [
    "Side note on generators:  Notice that kfold.split returns a generator object.  If you are not familiar with them, you can think of this as being similar to a list except that instead of storing all of the elements in memory it stores the current element and a rule for getting the next element.\n",
    "\n",
    "KFold is implemented this way to deal with memory issues if you use a large number of splits.  For instance, if a Leave Out One split was implemented as a list on a dataset of size $10000$ the size of the list would be $10000*9999$.  If you use a generator instead then at each stage you only need to keep a list of size $10000$ in memory, also remember which element you should leave out next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1adf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use for loop to demonstrate .split\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    print(\"Train index:\", train_index)\n",
    "    print(\"Test index:\", test_index)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## When fitting a model we'd do something like the following\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    ## get the kfold training data\n",
    "    X_train_train = X_train[train_index,:]\n",
    "    y_train_train = y_train[train_index]\n",
    "    \n",
    "    ## get the holdout data\n",
    "    X_holdout = X_train[test_index,:]\n",
    "    y_holdout = y_train[test_index]\n",
    "    \n",
    "    ## Then you'd fit your model\n",
    "    ## Then you'd record the error on the holdout set here\n",
    "#     model.fit(X_train_train, y_train_train)\n",
    "    \n",
    "#     error(y_holdout, model.predict(X_holdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0d1ab",
   "metadata": {},
   "source": [
    "### Validation set or cross-validation\n",
    "\n",
    "Cross-validation, when feasible, is preferred to a single validation set. In general it is better to have a collection of estimates of the generalization error instead of a single point estimate.\n",
    "\n",
    "However, it is not always feasible to perform cross-validation. Models that take prohibitively long to train limit the usefulness of cross-validation since $k$-fold cross-validation requires you to train the model $k$ distinct times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e870c74",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
