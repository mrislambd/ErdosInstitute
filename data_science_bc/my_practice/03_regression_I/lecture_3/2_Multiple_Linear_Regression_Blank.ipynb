{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1769a6",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the multiple linear regression model,\n",
    "- Show how to fit the model using the <i>normal equation</i>,\n",
    "- Fit a model with `sklearn`.\n",
    "- Show how to use the normal equations to make a custom class which mimics the sklearn model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f515b56",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2838277",
   "metadata": {},
   "source": [
    "## The multiple linear regression model\n",
    "\n",
    "Suppose there is a quantitative variable you want to predict/model called $y$ and a set of $p$ features $x_1, x_2, \\dots x_p$, then the multiple linear regression model regressing $y$ on $x_1, \\dots, x_p$ is:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_m x_p + \\epsilon = \\vec{x} \\cdot \\vec{\\beta} + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\beta_0, \\dots, \\beta_p \\in \\mathbb{R}$ are constants, $\\vec{\\beta}$ is the $(p+1)$-vector of the $\\beta_i$ in numerical order, \n",
    "\n",
    "$$\n",
    "\\vec{x} = \\left(1, x_1, x_2, \\dots, x_p \\right)^\\top,\n",
    "$$\n",
    "\n",
    "and $\\epsilon \\sim N(0,\\sigma)$ is an error term independent of $\\vec{x}$.  Note that we have \"padded\" $\\vec{x}$ with an initial one to capture the constant term.\n",
    "\n",
    "### Fitting the model\n",
    "\n",
    "Suppose that we have $n$ observations $(\\vec{x}_i, y_i)$.   We can package these into an $n \\times (p+1)$ matrix $X$ and a $n$-vector $\\vec{y}$\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & ... & x_{1p}\\\\\n",
    "1 & x_{21} & x_{22} & ... & x_{2p}\\\\\n",
    "  &        &        & \\vdots &    \\\\\n",
    "1 & x_{n1} & x_{n2} & ... & x_{np}\\\\  \n",
    "\\end{bmatrix}\n",
    "\n",
    "\\hphantom{fdsfds}\n",
    "\n",
    "\\vec{y} = \n",
    "\\begin{bmatrix}\n",
    "y_1\\\\y_2\\\\ \\vdots \\\\ y_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In order to fit a multiple linear regression model regressing $y$ on $\\vec{x}$ using this data we return to the mean square error.\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(\\vec{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - f_{\\vec{\\beta}}(\\vec{x}_i)\\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\vec{x}_i^\\top \\vec{\\beta}\\right)^2,\n",
    "$$\n",
    "\n",
    "we can rewrite this using some linear algebra as:\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(\\vec{\\beta})  = \\frac{1}{n} \\left\\vert \\vec{y} - X \\vec{\\beta}\\right\\vert^2\n",
    "$$\n",
    "\n",
    "We want to find the value of the parameter $\\vec{\\beta}$ which minimizes the MSE.\n",
    "\n",
    "As seen in Math Hour,  we can minimize it in two ways:\n",
    "\n",
    "* Using multivariable differential calculus:  find gradient of the MSE with respect of $\\beta$ and set it equal to zero.\n",
    "* Using linear algebra:  view this geometrically as projecting $\\vec{y}$ into the subspace spanned by the columns of $X$.\n",
    "\n",
    "Here is a short derivation using the linear algebra approach:  We are looking for $\\hat{\\beta}$ so that $\\vec{y} - X \\hat{\\beta}$ is orthogonal to $\\operatorname{Im}(X)$.  So \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& X^\\top (\\vec{y} - X \\hat{\\beta}) = 0\\\\\n",
    "& X^\\top \\vec{y} - X^\\top X \\hat{\\beta} = 0 \\\\\n",
    "& X^\\top X \\hat{\\beta} = X^\\top y\\\\\\\n",
    "& \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is the <i>ordinary least squares</i> estimate of the coefficient vector $\\hat{\\beta}$. Note that this formula is also sometimes called the <i>normal equation</i>.\n",
    "\n",
    "Note:  $X^\\top X$ is invertible if the columns of $X$ are linearly independent.  When this is not true we say that $X$ suffers from \"exact multicollinearity\".  Even if the columns of $X$ are linearly independent we can still have issues if they are \"close\" (i.e. if the condition number of $X$ is large)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81221a1e",
   "metadata": {},
   "source": [
    "### An example with synthetic data\n",
    "\n",
    "We will demonstrate how to fit this model using `sklearn` with some synthetic data.\n",
    "\n",
    "We will then write a naive custom class which functions similarly to the sklearn module to get a feel for what is going on \"under the hood\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5ae3c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "X = np.random.rand(1000,2)\n",
    "y = 4 + 2*X[:,0] + 3*X[:,1] + np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8823a31",
   "metadata": {},
   "source": [
    "### Using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fac5f6",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## import the LinearRegression object\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15145ce7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Make the model object\n",
    "\n",
    "reg = \n",
    "\n",
    "## Fit the model object\n",
    "## note I do NOT have to use reshape here\n",
    "## because X_train is a 2D np.array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be0d4a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## look at coef and intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac638d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Make a prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478ee19",
   "metadata": {},
   "source": [
    "### Implementing a custom linear regression class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da16f97",
   "metadata": {},
   "source": [
    "To give you a feeling for what is going on \"under the hood\" of an sklearn model object, we will write a minimal custom linear regression model object.  The actual sklearn implementation is much more sophisticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e170d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class CustomLinearRegression():\n",
    "    '''A minimal custom linear regression model object'''\n",
    "    def __init__(self):\n",
    "        self.beta = None\n",
    "    def fit(self,X,y):\n",
    "        '''Finds self.beta using the normal equation.  \n",
    "            X and y must be numpy arrays.\n",
    "            X must have shape (num_samples, num_features)\n",
    "            y must have shape (num_samples,)'''\n",
    "        X = np.hstack([np.ones((X.shape[0],1)), X]) # adds column of ones to X\n",
    "        XtX = \n",
    "        XtXinv = \n",
    "        Xty = \n",
    "        self.beta = \n",
    "    def predict(self,X):\n",
    "        X = np.hstack([np.ones((X.shape[0],1)), X]) # adds column of ones to X\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d5b4a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# instantiate model object\n",
    "custom_reg = \n",
    "\n",
    "# call .fit method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a40e82",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(f\"Our custom class found intercept {custom_reg.beta[0]:0.3f} and coefficients {custom_reg.beta[1]:0.3f} and {custom_reg.beta[2]:0.3f}\")\n",
    "print(f\"sklearn found intercept {reg.intercept_:0.3f} and coefficients {reg.coef_[0]:0.3f} and {reg.coef_[1]:0.3f}\")\n",
    "print('They are the same!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d1ea9",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
