{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "In this notebook are some exercises to gain more experience with the material presented in Lecture 3:  Simple Linear Regression.\n",
    "\n",
    "These problems provide some practice fitting models, and provide a stronger theoretical understanding of the technique as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Questions\n",
    "\n",
    "<b>Note: These questions cover some of the more theoretical concepts behind the algorithms/models we cover in the lecture videos/problem sessions. Sometimes these theoretical questions will require some knowledge of mathematics/statistics. If you approach a problem and you don't think that you have that knowledge yet, that's okay! You can always come back to that question at a later time :)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Deriving the Standard Error for $E(y|x=x^*)$\n",
    "\n",
    "Use what we learned in [MLE_simple_linear_regression](../../week_02_data-collection/math_hour_2/MLE_simple_linear_regression.ipynb) to find the standard error of $E(y|x=x^*)$.\n",
    "\n",
    "Recall that we established the following facts about linear regression:\n",
    "\n",
    "- $\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n \\left( x_i - \\overline{x} \\right)\\left( y_i - \\overline{y} \\right)}{\\sum_{i=1}^n \\left( x_i - \\overline{x} \\right)^2}$ \n",
    "\n",
    "- $\\hat{\\beta_0} = \\overline{y} - \\hat{\\beta_1} \\overline{x}$\n",
    "\n",
    "- $\\text{Var}(\\hat{\\beta_1}) = \\frac{\\sigma^2}{\\sum_{i=1}^n \\left( x_i - \\overline{x} \\right)^2}$\n",
    "\n",
    "- $\\text{Var}(\\hat{\\beta_0}) = \\frac{\\sigma^2}{n} + \\overline{x}^2 \\text{Var}(\\hat{\\beta_1})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Application of previous answer: Confidence Interval for the Regression Line\n",
    "\n",
    "Let's take a look back at the model we're fitting:\n",
    "$$\n",
    "y = f(x) + \\epsilon = \\beta_0 + \\beta_1x + \\epsilon,\n",
    "$$\n",
    "where $\\epsilon$ is a vector of independent $\\epsilon_i \\sim N(0,\\sigma^2)$ for all $i$.\n",
    "\n",
    "Now take the expectation on both sides:\n",
    "$$\n",
    "E(y) = E(\\beta_0 + \\beta_1x + \\epsilon) = \\beta_0 + \\beta_1E(x) + E(\\epsilon) = \\beta_0 + \\beta_1E(x),\n",
    "$$\n",
    "this is where we got the formula for $\\hat{\\beta_0}$. \n",
    "\n",
    "Now let's say we know the value of $x$ for example $x^*$, in probability terms we're now looking at $y$ conditional on $X=x^*$ (denoted $y|x=x^*$), and then take the expectation:\n",
    "$$\n",
    "E(y|x=x^*) =  \\beta_0 + \\beta_1E(x^*) = \\beta_0 + \\beta_1 x^*,\n",
    "$$\n",
    "because we are looking at a specific value of $X$ so it is no longer random. \n",
    "\n",
    "So the regression line we've been plotting is actually a series of point estimates for the mean value of $y$ given a specific value of $x$. We've been denoting these point estimates as $\\hat{y}$. \n",
    "\n",
    "Just like we gave a confidence interval for $\\beta_1$ using our point estimate $\\hat{\\beta_1}$ we can give a confidence interval for $y|x$ using our point estimate $\\hat{y}$. The formula for the confidence interval for $E(y|x=x^*)$ is:\n",
    "$$\n",
    "\\hat{y} \\pm t_{n-2,(1-\\alpha/2)}\\sqrt{\\frac{\\sum_{i=1}^n\\left(y_i - \\hat{y_i}\\right)^2}{n-2}}\\sqrt{\\frac{1}{n} + \\frac{\\left(x^* - \\overline{x}\\right)^2}{(n-1)s_x^2}} \\approx \\hat{y} \\pm t_{n-2,(1-\\alpha/2)}\\sqrt{MSE}\\sqrt{\\frac{1}{n} + \\frac{\\left(x^* - \\overline{x}\\right)^2}{(n-1)s_x^2}},\n",
    "$$\n",
    "where $n$ is the number of observations and $t_{n-2,(1-\\alpha/2)}$ is such that $P(T\\geq t_{n-2,(1-\\alpha/2)}) = \\alpha/2$ for a random variable $T$ with a Studentized $t$ distribution with $n-2$ degrees of freedom. This formula still follows the confidence interval pattern, where here the product of the square roots is the standard error of $E(y|x=x^*)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Prediction Intervals for SLR\n",
    "\n",
    "Recall our discussion on confidence intervals for $E(y|x=x^*)$.\n",
    "\n",
    "In addition to a confidence interval for the conditional mean, you can also produce what are known as prediction intervals for $y|x=x^*$, which give us a sense of what reasonable lower and upper bounds are for $y|x=x^*$ for a given confidence level, $1-\\alpha$.\n",
    "\n",
    "Recall that the $(1-\\alpha)$ confidence interval formula for $E(y|x=x^*)$ was given by:\n",
    "$$\n",
    "\\hat{y} \\pm t_{n-2,(1-\\alpha/2)}\\sqrt{\\frac{\\sum_{i=1}^n\\left(y_i - \\hat{y_i}\\right)^2}{n-2}}\\sqrt{\\frac{1}{n} + \\frac{\\left(x^* - \\overline{x}\\right)^2}{(n-1)s_x^2}},\n",
    "$$\n",
    "\n",
    "The formula for the $(1-\\alpha)$ prediction interval is quite similar:\n",
    "$$\n",
    "\\hat{y} \\pm t_{n-2,(1-\\alpha/2)}\\sqrt{\\frac{\\sum_{i=1}^n\\left(y_i - \\hat{y_i}\\right)^2}{n-2}}\\sqrt{1 + \\frac{1}{n} + \\frac{\\left(x^* - \\overline{x}\\right)^2}{(n-1)s_x^2}}.\n",
    "$$\n",
    "The addition of $1$ in the second square root refelects the extra uncertainty involved in predicting the actual $y$ value for a value of $x$, and comes from the error term in the statistical models, $\\epsilon$. This does not show up with the confidence interval because remember $E(\\bullet)$ is linear and $E(\\epsilon)$ is assumed to be $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Carefully considering your data\n",
    "\n",
    "Read through this excellent article on how survivor and selection bias can impact the results of your linear regression. <i>Note: the model used in the article is a nonlinear regression, but the findings would be similar for simple linear regression</i>.\n",
    "    \n",
    "<a href=\"https://fivethirtyeight.com/features/faster-nfl-prospects-arent-always-better/\">https://fivethirtyeight.com/features/faster-nfl-prospects-arent-always-better/</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Mean absolute error\n",
    "\n",
    "When solving for the \"best\" estimates of $\\beta_0$ and $\\beta_1$ we minimized the mean squared error (MSE). This results in what is known as the Ordinary Least Squares estimates which have a number of desirable properties under the assumptions of linear regression. Alternatively we could have tried to minimize the mean absolute error (MAE) which is given more generally by:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{f}(X_i)|.\n",
    "$$\n",
    "\n",
    "One reason that this is not typically done is that the absolute value is not differentiable everywhere, making optimization annoying. However, as a metric the MAE is more \"robust\", meaning that a regression line fit by minimizing the MAE is less likely to drastically move if an outlier is added to or removed from the data set. \n",
    "\n",
    "To see this we return to a supervised learning framework where we use $m$ features stored in $X$ to predict $y$ by estimating this model:\n",
    "\n",
    "$$\n",
    "y = f(X) + \\epsilon.\n",
    "$$\n",
    "\n",
    "When estimating $f$ using the squared errors we want to minimize:\n",
    "\n",
    "$$\n",
    "E\\left(y - f(X) \\right)^2.\n",
    "$$\n",
    "\n",
    "It turns out that this is minimized exactly when:\n",
    "\n",
    "$$\n",
    "f(X) = E(y|X).\n",
    "$$\n",
    "\n",
    "Conversely, when estimating $f$ using the absolute errors we want to minimize:\n",
    "\n",
    "$$\n",
    "E \\left( | y - f(X) | \\right),\n",
    "$$\n",
    "\n",
    "which is minimized when:\n",
    "\n",
    "$$\n",
    "f(X) = \\text{median}\\left(y | X \\right),\n",
    "$$\n",
    "\n",
    "for an argument of why this is true see <a href=\"visualizing_the_median.pdf\">Visualizing the Median as the Minimum-Deviation Location</a> in this folder.\n",
    "\n",
    "As a statistic, the median is more robust to outliers than the mean, which explains why minimizing the MAE leads to more \"robust\" regression than minimizing the MSE. This more robust version of regression can be implemented in `sklearn` with `RANSACRegressor`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Questions\n",
    "\n",
    "##### 1. Origins of Regression to the Mean.\n",
    "\n",
    "Load in the data set `galton.csv` located in the `data` folder.\n",
    "\n",
    "Create two subsets called `male` and `female`. \n",
    "\n",
    "Then reset the `father` variable in the `male` data set so that it is centered at the average `father` height. Do the same for the `mother` variable in the `female` data set.\n",
    "\n",
    "For the `male` data regress height on the father's height, for the female data regress height on the mother's height.\n",
    "\n",
    "Look at the estimates for $\\hat{\\beta_1}$ in both cases, what do these estimmates suggest about the height of the next generation?\n",
    "\n",
    "<i>Note that for this problem you do not need to worry about doing a train test split, this is because we will not be making a predictive model.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both estimates suggest that a child whose parent is tall is likely to be shorter than their parent, while a child whose parent is short is likely to be taller than their parent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. b) A Paradox?\n",
    "\n",
    "This phenomenon, where a child's height usually reverts to the population mean, would suggest that over time the population should converge to the average, right? \n",
    "\n",
    "What is incorrect with this line of reasoning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Estimating the standard deviation of $\\epsilon$.\n",
    "\n",
    "Recall that we assumed:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a normally distributed random variable with variance $\\sigma^2$.\n",
    "\n",
    "\n",
    "It is often useful for us to have an estimate of $\\sigma$ because it allows us to give reasonable guesses for the possible values of $y$ for a given value of $x$.\n",
    "\n",
    "We estimate $\\sigma$ by finding the standard deviation of the <i>residuals</i>, which are the predicted values minus the actual values or:\n",
    "\n",
    "$$\n",
    "\\hat{y} - y.\n",
    "$$\n",
    "\n",
    "So if we have $n$ observations we have the following estimate for $\\sigma$:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma} = \\sqrt{\\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-2}} \\approx RMSE\n",
    "$$\n",
    "\n",
    "Load in this phony data, fit a simple linear regression model on it and estimate $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 8*np.random.random(100) - 4\n",
    "y = 2*x + 3*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Simulating data using your regression estimates\n",
    "\n",
    "We will end with one way you can use this estimate of $\\sigma$.\n",
    "\n",
    "Sometimes you may like to simulate \"new observations\" that follow the model you fit. This is possible using the fit   coefficient estimates and the estimate of $\\sigma$. To simulate new observations we just recall the simple linear regression model:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon,\n",
    "$$\n",
    "\n",
    "which can be estimated with:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x + \\hat{\\epsilon}, \n",
    "$$\n",
    "\n",
    "where $\\hat{\\epsilon} \\sim N(0,\\hat{\\sigma}).$\n",
    "\n",
    "Simulate $100$ new observations of $y$ using the estimates you obtained in 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Regressing backwards\n",
    "\n",
    "What happens if we accidentally regress $x$ on $y$?\n",
    "\n",
    "a. Load the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,1000)\n",
    "y = x + np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. First, using either `numpy` or `sklearn` perform the regression correctly, meaning regress $y$ on $x$. Look at $\\hat{\\beta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Using either `numpy` or `sklearn` regress $x$ on $y$, then look at $\\hat{\\beta_1}$. Notice anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Now load in the new $y$ that has an $\\epsilon$ term with higher variance below, and refit the regression and look at $\\hat{\\beta_1}$. Notice anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2*np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Finish the `for` loop code that records the $\\hat{\\beta_1}$ for errors with increasing variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1_hats = []\n",
    "\n",
    "for c in np.linspace(.1,100,10000):\n",
    "    y = x + c*np.random.randn(1000)\n",
    "    \n",
    "    ## make a regression object here\n",
    "    \n",
    "    \n",
    "    \n",
    "    # fit the slr here\n",
    "    \n",
    "    \n",
    "    \n",
    "    # append the estimate to beta_1_hats here\n",
    "    beta_1_hats.append()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Plot the $\\hat{\\beta_1}$ as a function of `c`. What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n \\left( x_i - \\overline{x}\\right)\\left( y_i - \\overline{y} \\right)}{\\sum_{i=1}^n \\left(x_i - \\overline{x} \\right)^2} = \\frac{\\text{cov}(x,y)}{\\text{var}(x)},\n",
    "$$\n",
    "\n",
    "f. Recall the formula for $\\hat{\\beta_1}$ for a regression of $y$ on $x$ given above. If we assume that $y = \\beta_0 + \\beta_1 x + \\epsilon$ and then regress $x$ on $y$, what happens to the value of $\\hat{\\beta_1}$ as the variance of $\\epsilon$ goes to $\\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
